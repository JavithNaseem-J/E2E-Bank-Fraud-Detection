{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\E2E-Credit-Fraud-Detection\\\\Exp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\E2E-Credit-Fraud-Detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_path: Path\n",
    "    preprocess_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metrics_path: str\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.constants import *\n",
    "from project.utils.common import * \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_PATH,\n",
    "                 params_filepath = PARAMS_PATH,\n",
    "                 schema_filepath = SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.XGBClassifier\n",
    "        schema = self.schema.target_column\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_path=config.test_path,\n",
    "            model_path=config.model_path,\n",
    "            all_params = params,\n",
    "            preprocess_path=config.preprocess_path,\n",
    "            metrics_path=config.metrics_path,\n",
    "            target_column=schema.name\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import dagshub\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from project import logger\n",
    "from project.entity.config_entity import ModelEvaluationConfig\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "\n",
    "    def evaluation(self):\n",
    "        # Validate file paths\n",
    "        if not os.path.exists(self.config.test_path):\n",
    "            raise FileNotFoundError(f\"Test data file not found at {self.config.test_path}\")\n",
    "        if not os.path.exists(self.config.preprocess_path):\n",
    "            raise FileNotFoundError(f\"Preprocessor file not found at {self.config.preprocess_path}\")\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {self.config.model_path}\")\n",
    "\n",
    "\n",
    "        # Load preprocessor and model\n",
    "        logger.info(\"Loading preprocessor and model...\")\n",
    "        preprocessor = joblib.load(self.config.preprocess_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "\n",
    "        # Extract best estimator if model is RandomizedSearchCV\n",
    "        if hasattr(model, 'best_estimator_'):\n",
    "            logger.info(\"Model is a RandomizedSearchCV object, extracting best estimator...\")\n",
    "            best_params = model.best_params_\n",
    "            model = model.best_estimator_\n",
    "        else:\n",
    "            best_params = model.get_params()\n",
    "            logger.info(\"Model is a direct estimator, using its parameters...\")\n",
    "\n",
    "\n",
    "        # Load test and train data\n",
    "        test_data = pd.read_csv(self.config.test_path)\n",
    "        target_column = self.config.target_column\n",
    "\n",
    "        if target_column not in test_data.columns:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found in test data.\")\n",
    "\n",
    "        # Prepare test and train data\n",
    "        test_x = test_data.drop(columns=[target_column])\n",
    "        test_y = test_data[target_column]\n",
    "\n",
    "        test_x_preprocessed = preprocessor.transform(test_x)\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions = model.predict(test_x_preprocessed)\n",
    "\n",
    "        # Get predicted probabilities for ROC\n",
    "        test_probabilities = model.predict_proba(test_x_preprocessed)[:, 1]\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "                \"test_accuracy\": accuracy_score(test_y, test_predictions),\n",
    "                \"test_precision_weighted\": precision_score(test_y, test_predictions, average='weighted'),\n",
    "                \"test_recall_weighted\": recall_score(test_y, test_predictions, average='weighted'),\n",
    "                \"test_f1_weighted\": f1_score(test_y, test_predictions, average='weighted'),\n",
    "                \"test_auc\": auc(*roc_curve(test_y, test_probabilities)[:2])\n",
    "        }\n",
    "\n",
    "        \n",
    "        logger.info(f\"Model evaluation metrics: {metrics}\")\n",
    "\n",
    "        # Log confusion matrix\n",
    "        cm = confusion_matrix(test_y, test_predictions)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        cm_path = Path(self.config.root_dir)/\"cm.png\"\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Confusion matrix saved and logged at {cm_path}\")\n",
    "\n",
    "        # Log ROC curve\n",
    "        fpr, tpr, _ = roc_curve(test_y, test_probabilities)\n",
    "        roc_auc = metrics[\"test_auc\"]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\") \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        roc_path = Path(self.config.root_dir)/\"roc.png\"\n",
    "        plt.savefig(roc_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        logger.info(f\"ROC curve saved at {roc_path}\")\n",
    "\n",
    "\n",
    "        # Save and log metrics\n",
    "        metrics_file = Path(self.config.root_dir) / \"metrics.json\"\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "        logger.info(f\"Metrics saved to {metrics_file}\")\n",
    "\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 17:51:58,439: INFO: common: yaml file: yaml file\\config.yaml loaded successfully]\n",
      "[2025-04-21 17:51:58,461: INFO: common: yaml file: yaml file\\schema.yaml loaded successfully]\n",
      "[2025-04-21 17:51:58,466: INFO: common: yaml file: yaml file\\params.yaml loaded successfully]\n",
      "[2025-04-21 17:51:58,466: INFO: common: created directory at: artifacts]\n",
      "[2025-04-21 17:51:58,466: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-04-21 17:51:58,472: INFO: 3400358762: Loading preprocessor and model...]\n",
      "[2025-04-21 17:51:59,009: INFO: 3400358762: Model is a RandomizedSearchCV object, extracting best estimator...]\n",
      "[2025-04-21 17:51:59,115: INFO: 3400358762: Model evaluation metrics: {'test_accuracy': 0.9365979381443299, 'test_precision_weighted': 0.93986165708818, 'test_recall_weighted': 0.9365979381443299, 'test_f1_weighted': 0.9365057040580845, 'test_auc': np.float64(0.9769071388495418)}]\n",
      "[2025-04-21 17:51:59,310: INFO: 3400358762: Confusion matrix saved and logged at artifacts\\model_evaluation\\cm.png]\n",
      "[2025-04-21 17:51:59,496: INFO: 3400358762: ROC curve saved at artifacts\\model_evaluation\\roc.png]\n",
      "[2025-04-21 17:51:59,496: INFO: 3400358762: Metrics saved to artifacts\\model_evaluation\\metrics.json]\n",
      "[2025-04-21 17:51:59,496: INFO: 2568709446: Model evaluation completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.evaluation()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "finally:\n",
    "    logger.info(\"Model evaluation completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
